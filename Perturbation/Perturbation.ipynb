{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perturbation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this phase i'll face the problem of the data perturbation. \n",
    "I take the documents from dataset and perturbate them with different kind of error: \n",
    "\n",
    "1. Spelling Errors\n",
    "2. Grammatical Errors\n",
    "3. Word - Segmentation Errors\n",
    "4. Spurious - Character Insertion\n",
    "5. Mix of previous errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import random\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient(\"mongodb://localhost:27017\")\n",
    "db = client[\"vatican\"]\n",
    "dataset = db['texts']\n",
    "perturbedDataset = db['perturbedDataset']\n",
    "\n",
    "docs = list(dataset.find())[:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeDoc(doc):\n",
    "    item = {'docId': doc['_id'], 'type': doc['type'], 'sentences': []}\n",
    "    sentenceList = []\n",
    "\n",
    "    for j, s in enumerate(sent_tokenize(doc['text'].lower())): \n",
    "        sentenceList.append(word_tokenize(s))\n",
    "    item['sentences'] = sentenceList\n",
    "    \n",
    "    return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6ae3f06a0254a10ad3b653d27b74f9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "\n",
    "for i, d in tqdm(enumerate(docs)): \n",
    "    sentences.append(tokenizeDoc(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = {\n",
    "    'spurious': 0,\n",
    "    'spelling': 0, \n",
    "    'grammatical': 0, \n",
    "    'wordSegmentation': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spurious Character Insertion\n",
    "\n",
    "Insert of some random character in some random position inside some random picking word in the documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Potrebbe essere necessario diminuire il numero di frasi in cui viene inserito un errore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "characters = string.ascii_lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spurious(s):\n",
    "    if random.randint(0, 10) % 2 == 0:\n",
    "        return s\n",
    "\n",
    "    global errors\n",
    "    \n",
    "    posInSentence = random.randint(0, len(s) - 1) \n",
    "    posInWord = random.randint(0, len(s[posInSentence]) - 1)\n",
    "    w = s[posInSentence]\n",
    "    \n",
    "    if(len(s[posInSentence]) == 1):\n",
    "        return s\n",
    "    \n",
    "    s[posInSentence] = \"\".join((w[:posInWord], characters[random.randint(0, len(characters) - 1)], w[posInWord:]))\n",
    "    \n",
    "    errors['spurious'] += 1\n",
    "    \n",
    "    print('Spurious Character inserted')\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spelling Errors\n",
    "\n",
    "Are recurrents errors during the writing of a word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spelling(s):\n",
    "    if(random.randint(0, 10) % 2 == 0):\n",
    "        return s\n",
    "    \n",
    "    global errors\n",
    "    \n",
    "    posInSentence = random.randint(0, len(s) - 1) \n",
    "    posInWord = random.randint(0, len(s[posInSentence]) - 1)\n",
    "    w = s[posInSentence]\n",
    "\n",
    "    if(len(s[posInSentence]) == 1):\n",
    "        return s\n",
    "    \n",
    "    s[posInSentence] = \"\".join((w[:posInWord], characters[random.randint(0, len(characters) - 1)], w[posInWord+1:]))\n",
    "    \n",
    "    errors['spelling'] += 1\n",
    "    \n",
    "    print('Spelling Error inserted')\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grammatical Errors\n",
    "\n",
    "This kind of errors concern articles, aphostrophes and accents ecc. \n",
    "I've compiled a list of possible grammatical errors (in form of dictionary). When i found a word / phrase / character that contains the correct version of the error i replace the correct version with the wrong version (obviously not for each occurence). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grammaticalErrorsSet():\n",
    "    gErrorsWords = {\n",
    "        'ha': ['a'],\n",
    "        'hanno': ['anno'], \n",
    "        'ho': ['o'],\n",
    "        'hai': ['ai']\n",
    "        }\n",
    "    gErrorsChar = {\n",
    "        'ù': ['u', 'u\\''],\n",
    "        'è': ['e', 'e\\''],\n",
    "        'é': ['e', 'è', 'e\\''],\n",
    "        'ò': ['o', 'o\\''], \n",
    "        'à': ['a', 'a\\''],\n",
    "        'ì': ['i', 'i\\''],\n",
    "        '\\'':  ['']\n",
    "    }\n",
    "    \n",
    "    return gErrorsWords, gErrorsChar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def grammatical(s):\n",
    "    global errors\n",
    "    \n",
    "    gErrorsWords, gErrorsChar = grammaticalErrorsSet()\n",
    "    \n",
    "    for k, w in enumerate(s):\n",
    "        if(random.randint(0, 100) < 80):\n",
    "            return s\n",
    "        \n",
    "        if len(w) == 1:\n",
    "            return s\n",
    "        \n",
    "        for err in gErrorsWords: \n",
    "            if w == err: \n",
    "                s[k] = (gErrorsWords.get(err))[random.randint(0, len(gErrorsWords.get(err)) - 1)]\n",
    "                \n",
    "                print('Grammatical Error inserted')\n",
    "                \n",
    "                errors['grammatical'] += 1\n",
    "\n",
    "        for err in gErrorsChar:\n",
    "            ms = re.finditer(unicodedata.normalize('NFKD', err), w)\n",
    "            mp = [m.start() for m in ms ]\n",
    "\n",
    "            if len(mp) > 0:\n",
    "                pos = mp[random.randint(0, len(mp) - 1)] if (len(mp) > 1) else mp[0]\n",
    "                item = random.randint(0, len(gErrorsChar.get(err)) - 1)\n",
    "\n",
    "                s[k] = \"\".join(\n",
    "                    (w[:pos],\n",
    "                    gErrorsChar.get(err)[0] if (len(err) == 0) else gErrorsChar.get(err)[item],\n",
    "                    w[pos+1:])\n",
    "                )\n",
    "                \n",
    "                print('Grammatical Error inserted')\n",
    "                \n",
    "                errors['grammatical'] += 1\n",
    "            \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Segmentation Error\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordSegmentation(s):\n",
    "    print('Word Segmentation Error inserted')\n",
    "    \n",
    "    #inserire conteggio errori\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4633a2134be249348b232272c8f2efff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spelling Error inserted\n",
      "Word Segmentation Error inserted\n",
      "Word Segmentation Error inserted\n",
      "Spelling Error inserted\n",
      "Word Segmentation Error inserted\n",
      "Spurious Character inserted\n",
      "Word Segmentation Error inserted\n",
      "Spurious Character inserted\n",
      "Spelling Error inserted\n",
      "Word Segmentation Error inserted\n",
      "Spelling Error inserted\n",
      "Word Segmentation Error inserted\n",
      "Spurious Character inserted\n",
      "Word Segmentation Error inserted\n",
      "Word Segmentation Error inserted\n",
      "Spurious Character inserted\n",
      "Spelling Error inserted\n",
      "Word Segmentation Error inserted\n",
      "Spurious Character inserted\n",
      "Word Segmentation Error inserted\n",
      "Word Segmentation Error inserted\n",
      "Spelling Error inserted\n",
      "Word Segmentation Error inserted\n",
      "Spurious Character inserted\n",
      "Word Segmentation Error inserted\n",
      "Spurious Character inserted\n",
      "Spelling Error inserted\n",
      "Word Segmentation Error inserted\n",
      "Spelling Error inserted\n",
      "Word Segmentation Error inserted\n",
      "Spurious Character inserted\n",
      "Spelling Error inserted\n",
      "Word Segmentation Error inserted\n",
      "Spurious Character inserted\n",
      "Word Segmentation Error inserted\n",
      "Spurious Character inserted\n",
      "Spelling Error inserted\n",
      "Word Segmentation Error inserted\n",
      "Word Segmentation Error inserted\n",
      "Word Segmentation Error inserted\n",
      "Spelling Error inserted\n",
      "Word Segmentation Error inserted\n",
      "Word Segmentation Error inserted\n",
      "Word Segmentation Error inserted\n",
      "Word Segmentation Error inserted\n",
      "Spurious Character inserted\n",
      "Word Segmentation Error inserted\n",
      "Word Segmentation Error inserted\n",
      "Word Segmentation Error inserted\n",
      "Word Segmentation Error inserted\n",
      "Word Segmentation Error inserted\n",
      "Spurious Character inserted\n",
      "Word Segmentation Error inserted\n",
      "Spelling Error inserted\n",
      "Word Segmentation Error inserted\n",
      "Word Segmentation Error inserted\n",
      "Word Segmentation Error inserted\n",
      "Spurious Character inserted\n",
      "Spelling Error inserted\n",
      "Word Segmentation Error inserted\n",
      "Word Segmentation Error inserted\n",
      "Spurious Character inserted\n",
      "Word Segmentation Error inserted\n",
      "Spurious Character inserted\n",
      "Word Segmentation Error inserted\n",
      "Word Segmentation Error inserted\n",
      "Word Segmentation Error inserted\n",
      "Spurious Character inserted\n",
      "Word Segmentation Error inserted\n",
      "Spurious Character inserted\n",
      "Word Segmentation Error inserted\n",
      "Spurious Character inserted\n",
      "Word Segmentation Error inserted\n",
      "Spurious Character inserted\n",
      "Word Segmentation Error inserted\n",
      "Spurious Character inserted\n",
      "Word Segmentation Error inserted\n",
      "Spelling Error inserted\n",
      "Word Segmentation Error inserted\n",
      "Spurious Character inserted\n",
      "Word Segmentation Error inserted\n",
      "Word Segmentation Error inserted\n",
      "Word Segmentation Error inserted\n",
      "Word Segmentation Error inserted\n",
      "Word Segmentation Error inserted\n",
      "Spelling Error inserted\n",
      "Word Segmentation Error inserted\n",
      "Word Segmentation Error inserted\n",
      "Spurious Character inserted\n",
      "Word Segmentation Error inserted\n",
      "Word Segmentation Error inserted\n",
      "Spurious Character inserted\n",
      "Word Segmentation Error inserted\n",
      "Spelling Error inserted\n",
      "Word Segmentation Error inserted\n",
      "Spurious Character inserted\n",
      "Word Segmentation Error inserted\n",
      "Spurious Character inserted\n",
      "Spelling Error inserted\n",
      "Word Segmentation Error inserted\n",
      "Word Segmentation Error inserted\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, d in tqdm(enumerate(sentences)):\n",
    "    listOfPerturbation = []\n",
    "    for j, s in enumerate(d['sentences']):\n",
    "        gT = s.copy()\n",
    "        wordSegmentation(grammatical(spelling(spurious(s))))\n",
    "        \n",
    "        listOfPerturbation.append([s, gT])\n",
    "    item = {'docId': d['docId'], 'type': d['type'], 'sentences': listOfPerturbation}\n",
    "    perturbedDataset.insert_one(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERRORS: 2915\n",
      "SPURIOUS: 1404\n",
      "SPELLING: 1411\n",
      "GRAMMATICAL: 100\n",
      "WORD SEGMENTATION: 0\n",
      "Parole totali:  124198\n",
      "Error rate:  0.023470587288040065\n"
     ]
    }
   ],
   "source": [
    "print(\"ERRORS: {}\\nSPURIOUS: {}\\nSPELLING: {}\\nGRAMMATICAL: {}\\nWORD SEGMENTATION: {}\".format(\n",
    "    sum(errors.values()),\n",
    "    errors.get('spurious'), errors.get('spelling'), errors.get('grammatical'), errors.get('wordSegmentation'))\n",
    ")\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for d in sentences:\n",
    "    for s in d['sentences']:\n",
    "        for w in s:\n",
    "            counter += 1\n",
    "            \n",
    "print(\"Parole totali: \", counter)\n",
    "print(\"Error rate: \", sum(errors.values())/counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-2ee461b460ad>:1: DeprecationWarning: count is deprecated. Use estimated_document_count or count_documents instead. Please note that $where must be replaced by $expr, $near must be replaced by $geoWithin with $center, and $nearSphere must be replaced by $geoWithin with $centerSphere\n",
      "  print(perturbedDataset.count())\n"
     ]
    }
   ],
   "source": [
    "print(perturbedDataset.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(False):\n",
    "    perturbedDataset.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
