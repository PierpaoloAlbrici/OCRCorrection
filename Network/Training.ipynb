{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "In this notebook i'll going to pre process the training dataset and then to train the Network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import string\n",
    "import nltk\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient('mongodb://localhost:27017')\n",
    "db = client['vatican']\n",
    "dataset = db['wordsDataset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(s):\n",
    "    out = \"\"\n",
    "    for c in s.lower():\n",
    "        if c in CHAR_INDEX.keys():\n",
    "            out += c\n",
    "        else:\n",
    "            out += '#'\n",
    "            \n",
    "    if(len(s) == 1):\n",
    "        return '#'\n",
    "    \n",
    "    return \"\".join(out)\n",
    "\n",
    "def string_to_matrix(s, n=2):\n",
    "    z = preprocess(s)\n",
    "    C = np.zeros((len(CHAR_INDEX), len(CHAR_INDEX)))\n",
    "    for a, b in nltk.ngrams(z, n=n):\n",
    "        C[CHAR_INDEX[a], CHAR_INDEX[b]] += 1\n",
    "        \n",
    "    #print(C, C.max())\n",
    "    #Add one to avoid division by zero\n",
    "    C /= C.max()\n",
    "    \n",
    "    return C\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-f2e773a62c42>:1: DeprecationWarning: count is deprecated. Use estimated_document_count or count_documents instead. Please note that $where must be replaced by $expr, $near must be replaced by $geoWithin with $center, and $nearSphere must be replaced by $geoWithin with $centerSphere\n",
      "  print(dataset.count())\n"
     ]
    }
   ],
   "source": [
    "print(dataset.count())\n",
    "word = list(dataset.find())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4cd07dd8af146d9a71df609bf38d08d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=14564.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "wList = []\n",
    "wCorrectList = []\n",
    "for w in tqdm(word): \n",
    "    if w['word'] not in wList: \n",
    "        wList.append(w['word'])\n",
    "    if w['word'] not in wCorrectList:\n",
    "        wCorrectList.append(w['word'])\n",
    "    for o in w['occurrences']:\n",
    "        if o not in wList: \n",
    "            wList.append(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5122\n"
     ]
    }
   ],
   "source": [
    "for i, w in enumerate(wCorrectList):\n",
    "    if w == '19##':\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14417\n",
      "16838\n"
     ]
    }
   ],
   "source": [
    "print(len(wCorrectList))\n",
    "print(len(wList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = ''\n",
    "\n",
    "for w in wList: \n",
    "    for c in w: \n",
    "        if chars.count(c) == 0: \n",
    "            chars = chars+(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n"
     ]
    }
   ],
   "source": [
    "CHAR_INDEX = dict((c, i) for i, c in enumerate(chars))\n",
    "print(len(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, testing_data = [], []\n",
    "\n",
    "for w in word:\n",
    "    training_list, testing_list = [], []\n",
    "    \n",
    "    wrd = w['word']\n",
    "    for i, sw in enumerate(w['occurrences']):\n",
    "        if(wrd != sw): \n",
    "            if(i < 5):\n",
    "                training_list.append(sw)\n",
    "            else: \n",
    "                testing_list.append(sw)\n",
    "\n",
    "    training_data.append((training_list, wrd))\n",
    "    testing_data.append((testing_list, wrd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING DATA:  1915\n",
      "[(0, 13253), (1, 996), (2, 145), (3, 51), (4, 119)]\n",
      "TESTING DATA:  628\n",
      "[(0, 14479), (1, 17), (2, 12), (3, 8), (4, 11), (5, 9), (6, 3), (7, 2), (8, 5), (9, 2), (10, 1), (11, 2), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (19, 1), (20, 1), (27, 1), (29, 1), (37, 1), (41, 1), (42, 1), (67, 1)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print('TRAINING DATA: ', sum([len(x[0]) for x in training_data]))\n",
    "\n",
    "training_stats = defaultdict(lambda: 0)\n",
    "\n",
    "for t in training_data: \n",
    "    training_stats[len(t[0])] += 1\n",
    "\n",
    "print(sorted(training_stats.items()))\n",
    "\n",
    "print('TESTING DATA: ', sum([len(x[0]) for x in testing_data]))\n",
    "\n",
    "testing_stats = defaultdict(lambda: 0)\n",
    "\n",
    "for t in testing_data:\n",
    "    testing_stats[len(t[0])] += 1\n",
    "\n",
    "print(sorted(testing_stats.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x22544ad4ed0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_LABELS = len(wCorrectList)\n",
    "V = len(CHAR_INDEX) * len(CHAR_INDEX)\n",
    "LABEL_INDEX = dict((l, i) for i, l in enumerate(wCorrectList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simple2Gram(nn.Module):\n",
    "    def __init__(self, num_labels, size):\n",
    "        super(Simple2Gram, self).__init__()\n",
    "        self.linear = nn.Linear(size, num_labels)\n",
    "    \n",
    "    def forward(self, vec):\n",
    "        return F.log_softmax(self.linear(vec), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector(s, n=2):\n",
    "    vec = torch.tensor(string_to_matrix(s, n=n)).float()\n",
    "    return vec.view(1, -1)\n",
    "\n",
    "def target(label):\n",
    "    return torch.LongTensor([LABEL_INDEX[label]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Simple2Gram(N_LABELS, V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0218,  0.0237, -0.0067,  ...,  0.0085, -0.0187,  0.0139],\n",
      "        [-0.0244,  0.0246,  0.0277,  ...,  0.0193, -0.0028, -0.0009],\n",
      "        [-0.0021,  0.0238,  0.0162,  ...,  0.0167, -0.0174,  0.0055],\n",
      "        ...,\n",
      "        [-0.0094, -0.0076,  0.0134,  ..., -0.0046, -0.0160, -0.0158],\n",
      "        [ 0.0189, -0.0012,  0.0063,  ...,  0.0042,  0.0211, -0.0120],\n",
      "        [-0.0124, -0.0231,  0.0199,  ..., -0.0205, -0.0120, -0.0228]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0044, -0.0169, -0.0175,  ...,  0.0158,  0.0195, -0.0231],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters(): \n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    sample = training_data[0]\n",
    "    vec = vector(sample[0][0], n = 2)\n",
    "    log_probs = model(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 14417])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d0608e0370e4252ad434d147ae541c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38e1f3f26b9d49f2a0455852b32cfd8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=14564.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(1)): #50\n",
    "    for samples, label in tqdm(training_data):\n",
    "        for instance in samples:\n",
    "            #print(instance, samples, label)\n",
    "            model.zero_grad()\n",
    "            vec = vector(instance)\n",
    "            tar = target(label)\n",
    "            log_probs = model(vec)\n",
    "            #print(log_probs)\n",
    "            L = loss(log_probs, tar)\n",
    "            L.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PATH = \"C:\\Users\\piera\\Desktop\\Repository\\VaticanGenderEvolution\\models\"\n",
    "#PATH = \"C:\\Users\\piera\\Desktop\"\n",
    "torch.save(model, '.\\models\\model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.distributions import Categorical \n",
    "\n",
    "predictions, y_true = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for test, label in testing_data:\n",
    "        for word in test: \n",
    "            vec = vector(word, n=2)\n",
    "            log_probs = model(vec)\n",
    "            print('Input String', word)\n",
    "            prediction = np.argmax(log_probs.numpy())\n",
    "            print('Guess:', wCorrectList[prediction], '(',label ,')' '\\n')\n",
    "            \n",
    "            predictions.append(model(vec))\n",
    "            y_true.append(label)\n",
    "\n",
    "y_true = np.array(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#entropy = lambda x: Categorical(probs = x).entropy()\n",
    "#E = sum([entropy(p) for p in predictions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [wCorrectList[np.argmax(p.numpy())] for p in predictions]\n",
    "#y_pred = [np.argmax(p.numpy()) for p in predictions]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "lst = []\n",
    "for i in y_true:\n",
    "    lst.append(type(i))\n",
    "print(set(lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(multilabel_confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
